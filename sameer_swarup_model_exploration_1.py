# -*- coding: utf-8 -*-
"""Sameer Swarup - Model Exploration 1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Wec7OU-Tv5uXKNYrY--K8D8t02VdigNi
"""

# Run this box to mount your drive, which will allow you to load data and/or
# modules stored on your drive or in our shared drive
from google.colab import drive
#force_remount = True means that if you run this cell again, it will re-load your
#drive. That's necessary because otherwise, if you make changes to files in Drive,
#you won't see them in Colab - you have to re-run this cell to load the changes.
drive.mount('/content/drive', force_remount=True)

# Paths to the training and test data
# The column labeled povertyclassification is what we're trying to predict.
# The other columns are features. Look at:
# drive/Shared drives/CS320MachineLearning/data/modelExploration1/costa_rica_poverty_codebook.csv
# for the meanings of the column labels.
#
training_data_path = "drive/Shared drives/CS320MachineLearning/data/modelExploration1/costa_rica_poverty_train.csv"
test_data_path = "drive/Shared drives/CS320MachineLearning/data/modelExploration1/costa_rica_poverty_test.csv"
# This is the names of the features, broken down into real-valued versus categorical.
# You are only required to deal with categorical features, although one extension
# is to consider real-valued features as well. 
real_valued_features = ['rooms', '<12 male', '>= 12 male',
       'total males', '<12 female', '>=12 female', 'total females', '<12',
       '>12','escolari', 'hhsize', 'hogar_nin',
       'hogar_adul', 'hogar_mayor', 'hogar_total', 
       'bedrooms', 'overcrowding','qmobilephone','age']
categorical_features = ['v14a', 'refrig', 'v18q', 'pisomoscer', 'pisocemento', 'pisoother',
       'pisonatur', 'pisonotiene', 'pisomadera', 'techozinc', 'techoentrepiso',
       'techocane', 'techootro', 'cielorazo', 'abastaguadentro',
       'abastaguafuera', 'abastaguano', 'public', 'planpri', 'noelec',
       'coopele', 'sanitario1', 'sanitario2', 'sanitario3', 'sanitario5',
       'sanitario6', 'energcocinar1', 'energcocinar2', 'energcocinar3',
       'energcocinar4', 'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4',
       'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3', 'etecho1',
       'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', 'dis', 'male',
       'female', 'estadocivil2', 'estadocivil3', 'estadocivil4','estadocivil5',
       'estadocivil6', 'estadocivil7','educationlevel','tipovivi1', 'tipovivi2', 
       'tipovivi3', 'tipovivi4', 'tipovivi5', 'computer', 'television','lugar1', 
       'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6', 'area1', 'area2']

# Paths to the datafiles with the original categores (before I converted the problem
# to binary classification). These may be helpful for error analysis - e.g., do you perform
# better on the higher-need households that were most high need?
training_data_original_categories_path = "drive/Shared drives/CS320MachineLearning/data/modelExploration1/costa_rica_poverty_orig_cats_train.csv"
test_data_original_categories_path = "drive/Shared drives/CS320MachineLearning/data/modelExploration1/costa_rica_poverty_orig_cats_test.csv"

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

class NaiveBayes:
  '''
  Build a Naive Bayesian model that takes in a list of categorical features
  '''
  categorical_features = []
  classesDict = {}
  classesDict[0] = {}
  classesDict[1] = {}

  def __init__(self, categorical_features):
    self.categorical_features = categorical_features

  def trainingHelper(self, df, target, additive_smoothing_value, class_key, feature_name):
    #Helper method for train
    #Returns a dictionary with the probability for a feature class combination
    df_filtered = df.filter(items = [feature_name, target])
    df_filtered = df_filtered[df_filtered[target] == class_key]

    count = 0
    for i, row in df_filtered.iterrows():
      if row[feature_name] == 1:
        count += 1

    theta_c_k = (count + additive_smoothing_value)/(df_filtered.shape[0] + 2*additive_smoothing_value)
    self.classesDict[class_key][feature_name] = theta_c_k

  def train(self, df, target, additive_smoothing_value):
    #Assigns dictionary returned from helper method to a feature class combination
    #Finds the overall probability for each class
    for class_key in range(0,2):
      for feature in self.categorical_features:
        self.trainingHelper(df, target, additive_smoothing_value, class_key, feature)

    for key in self.classesDict.keys():
      self.classesDict[key]["class probability"] = len(df[df[target] == key])/len(df[target])
               
  def get_probability(self, feature_name, feature_value, class_value):
    if feature_name not in self.categorical_features:
      return 1

    if feature_value == 1:
      probability = self.classesDict[class_value][feature_name]
      return probability
    else:
      probability = 1 - self.classesDict[class_value][feature_name]
      return probability
    
  def get_class_probability(self, class_value):
    class_probability = self.classesDict[class_value]["class probability"]
    return class_probability

  def test(self, datapoint):
    #Returns the class that maximizes the posterior distribution
    log_prob_class_0 = np.log(self.get_class_probability(0))
    log_prob_class_1 = np.log(self.get_class_probability(1))
    summation_0 = 0
    summation_1 = 0

    for key,value in datapoint.items():
      summation_0 = summation_0 + np.log(self.get_probability(key, value, 0))

    for key,value in datapoint.items():
      summation_1 = summation_1 + np.log(self.get_probability(key, value, 1))

    total_0 = log_prob_class_0 + summation_0
    total_1 = log_prob_class_1 + summation_1

    if total_0 > total_1:
      return 0

    else:
      return 1

#Training Data
df_train = pd.read_csv("drive/Shared drives/CS320MachineLearning/data/modelExploration1/costa_rica_poverty_train.csv")
df_train.shape

#Test data
df_test = pd.read_csv("drive/Shared drives/CS320MachineLearning/data/modelExploration1/costa_rica_poverty_test.csv")
df_test_drop = df_test.drop("povertyclassification", axis=1)
labels = df_test["povertyclassification"]

#Splitting training data into training and development sets
from sklearn.model_selection import train_test_split
df_train_split, df_dev_split = train_test_split(df_train, test_size = 0.2)

print(df_train_split.shape)
print(df_dev_split.shape)

def get_metric(dataset, model):
  '''
  Returns accuracy, sensitivity, specificity and precision
  '''

  if dataset == "train":
    df = df_train_split
    num_households = df.shape[0]

  elif dataset == "test":
    df = df_test
    num_households = df.shape[0]

  elif dataset == "dev":
    df = df_dev_split
    num_households = df.shape[0]

  else:
    print("Please input a correct dataset.")

  num_correct = 0
  true_positives = 0
  true_negatives = 0
  false_positives = 0
  false_negatives = 0
  results = []
  truths = []

  for index, row in df.iterrows():
    household = row
    datapoint = {}
    for feature in model.categorical_features:
      datapoint[feature] = household[feature]
    results.append(model.test(datapoint))
    truths.append(household["povertyclassification"])


  for i in range(len(results)):
    if results[i] == truths[i]:
      num_correct += 1
      if truths[i] == 0:
        true_positives += 1
      else:
        true_negatives += 1

    else:
      if truths[i] == 0:
        false_negatives += 1
      else:
        false_positives += 1

  accuracy = num_correct/num_households
  sensitivity = true_positives/(true_positives + false_negatives)
  specificity = true_negatives/(true_negatives + false_negatives)
  precision = true_positives/(true_positives + false_positives)

  return accuracy,sensitivity, specificity, precision

#Get overall accuracy, sensitivity, specificity and precision

model = NaiveBayes(categorical_features)


model.train(df_train, "povertyclassification", 0)
accuracy, sensitivity, specificity, precision = get_metric("test", model)

print(accuracy)
print(sensitivity)
print(specificity)
print(precision)

#Look at number of true positives for each categorical feature
ax = plt.gca()
ax.axes.get_xaxis().set_visible(False)
plt.plot(categorical_features, true_positive_list, label = "True positives for each feature")
plt.title("True positives for each feature")

#Find all rare feature class combinations
rare_features = []
for feature in categorical_features:
  if len((df_train[df_train[feature] == 1])) < 10:
    rare_features.append(feature)

for feature in categorical_features:
  if len((df_train[df_train[feature] == 0])) < 10:
    rare_features.append(feature)

print(rare_features)

#Changing features
df_train_split['electricity'] = np.where(((df_train_split['public'] == 1) | (df_train_split['planpri'] == 1) | (df_train_split['coopele'] == 1)) & (df_train_split['noelec'] == 0), 1, 0)
df_dev_split['electricity'] = np.where((df_dev_split['public'] == 1) | (df_dev_split['planpri'] == 1) | (df_dev_split['coopele'] == 1) | (df_dev_split['noelec'] == 0), 1, 0)

df_train_split['qmobilephone'] = np.where((df_train_split['qmobilephone'] > 0), 1, 0)
df_dev_split['qmobilephone'] = np.where((df_dev_split['qmobilephone'] > 0), 1, 0)
                                          
df_train_split['elimbasu_other'] = np.where((df_train_split['elimbasu4'] == 1) | (df_train_split['elimbasu5'] == 1) | (df_train_split['elimbasu6'] == 1), 1, 0)
df_dev_split['elimbasu_other'] = np.where((df_dev_split['elimbasu4'] == 1) | (df_dev_split['elimbasu5'] == 1) | (df_dev_split['elimbasu6'] == 1), 1, 0)

df_train_split['piso_other'] = np.where((df_train_split['pisoother'] == 1) | (df_train_split['pisonatur'] == 1), 1, 0)
df_dev_split['piso_other'] = np.where((df_dev_split['pisoother'] == 1) | (df_dev_split['pisonatur'] == 1), 1, 0)

df_train_split['techo_other'] = np.where((df_train_split['techocane'] == 1) | (df_train_split['techootro'] == 1), 1, 0)
df_dev_split['techo_other'] = np.where((df_dev_split['techootro'] == 1) | (df_dev_split['techocane'] == 1), 1, 0)

#Groups of categorical features
appliances_categorical_features = ["refrig", "computer", "television", "qmobilephone", "v14a", "v18q"]

utilities_categorical_features = ["abastaguadentro", "abastaguafuera", "abastaguano", 'electricity', 
                                  "sanitario1", "sanitario2", "sanitario3", "sanitario5", "sanitario6", 
                                  "energcocinar1", "energcocinar2", "energcocinar3", "energcocinar4",
                                  "elimbasu1", "elimbasu2", "elimbasu3", "elimbasu_other"]

family_categorical_features = ['dis', 'male','female', 'estadocivil2', 'estadocivil3', 
                              'estadocivil4','estadocivil5','estadocivil6', 'estadocivil7']

materials_categorical_features = ['pisomoscer', 'pisocemento', 'piso_other','pisonotiene', 'pisomadera',
                                  'techozinc', 'techoentrepiso', 'techo_other', 'cielorazo','epared1', 'epared2', 
                                  'epared3', 'etecho1','etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3']

ownership_categorical_features = ['tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5',]

regional_categorical_features = ['lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6', 
                                 'area1', 'area2']

#Evaluating effectiveness of using electricity as a categorical feature
model = NaiveBayes(['electricity'])
model.train(df_train_split, "povertyclassification", 0)
accuracy, sensitivity, specificity, precision = get_metric('dev', model)
print(accuracy)
print(sensitivity)
print(specificity)
print(precision)

#Investigate impact of smoothing on viability of using electricity as a feature
accuracy_electricity = []
sensitivity_electricity = []
specificity_electricity = []
precision_electricity = []
smoothing_list = []

model = NaiveBayes(['electricity'])

for i in range(1, 100, 10):
  model.train(df_train_split, "povertyclassification", i)
  accuracy, sensitivity, specificity = get_metric('dev', model)
  accuracy_electricity.append(accuracy)
  sensitivity_electricity.append(sensitivity)
  specificity_electricity.append(specificity)
  smoothing_list.append(i)

plt.plot(smoothing_list, accuracy_electricity,'b-', label = "Accuracy")
plt.plot(smoothing_list, sensitivity_electricity,'r-', label = "Recall")
plt.legend(loc = 'best')
plt.title("Variation of accuracy, recall and specificity with smoothing for electricity")
plt.show()

#Look at utilities
model = NaiveBayes(utilities_categorical_features)
model.train(df_train_split, "povertyclassification", 0)
accuracy, sensitivity, specificity = get_metric("dev", model)
print(accuracy)
print(sensitivity)
print(specificity)

#Look at appliances
model = NaiveBayes(appliances_categorical_features)
model.train(df_train_split, "povertyclassification", 0)
accuracy, sensitivity, specificity = get_metric("dev", model)
print(accuracy)
print(sensitivity)
print(specificity)

#Look at family status
model = NaiveBayes(family_categorical_features)
model.train(df_train_split, "povertyclassification", 0)
accuracy, sensitivity, specificity = get_metric("dev", model)
print(accuracy)
print(sensitivity)
print(specificity)

#Look at construction material
model = NaiveBayes(materials_categorical_features)
model.train(df_train_split, "povertyclassification", 0)
accuracy, sensitivity, specificity = get_metric("dev", model)
print(accuracy)
print(sensitivity)
print(specificity)

#Look at ownership status
model = NaiveBayes(ownership_categorical_features)
model.train(df_train_split, "povertyclassification", 0)
accuracy, sensitivity, specificity = get_metric("dev", model)
print(accuracy)
print(sensitivity)
print(specificity)

#Look at household location
model = NaiveBayes(regional_categorical_features)
model.train(df_train_split, "povertyclassification", 0)
accuracy, sensitivity, specificity = get_metric("dev", model)
print(accuracy)
print(sensitivity)
print(specificity)